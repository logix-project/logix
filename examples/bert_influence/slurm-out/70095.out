no change     /home/hahn2/mambaforge/condabin/conda
no change     /home/hahn2/mambaforge/bin/conda
no change     /home/hahn2/mambaforge/bin/conda-env
no change     /home/hahn2/mambaforge/bin/activate
no change     /home/hahn2/mambaforge/bin/deactivate
no change     /home/hahn2/mambaforge/etc/profile.d/conda.sh
no change     /home/hahn2/mambaforge/etc/fish/conf.d/conda.fish
no change     /home/hahn2/mambaforge/shell/condabin/Conda.psm1
no change     /home/hahn2/mambaforge/shell/condabin/conda-hook.ps1
no change     /home/hahn2/mambaforge/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/hahn2/mambaforge/etc/profile.d/conda.csh
modified      /home/hahn2/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Added mamba to /home/hahn2/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Loaded model from /data/tir/projects/tir6/general/hahn2/analog/examples/bert_influence/files/checkpoints/0/sst2_epoch_3.pt.
[2023-11-23 11:10:06] [WARNING] Log directory ./log already exists.

[2023-11-23 11:10:06] [INFO] Tracking the following modules:
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.0.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.0.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.0.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.0.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.0.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.0.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.1.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.1.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.1.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.1.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.1.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.1.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.2.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.2.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.2.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.2.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.2.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.2.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.3.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.3.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.3.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.3.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.3.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.3.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.4.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.4.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.4.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.4.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.4.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.4.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.5.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.5.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.5.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.5.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.5.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.5.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.6.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.6.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.6.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.6.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.6.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.6.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.7.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.7.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.7.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.7.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.7.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.7.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.8.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.8.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.8.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.8.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.8.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.8.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.9.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.9.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.9.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.9.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.9.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.9.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.10.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.10.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.10.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.10.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.10.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.10.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.11.attention.self.query: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.11.attention.self.key: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.11.attention.self.value: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.11.attention.output.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.11.intermediate.dense: Linear(in_features=768, out_features=3072, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.encoder.layer.11.output.dense: Linear(in_features=3072, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.bert.pooler.dense: Linear(in_features=768, out_features=768, bias=True)
[2023-11-23 11:10:06] [INFO] model.classifier: Linear(in_features=768, out_features=2, bias=True)
[2023-11-23 11:10:06] [INFO] Total number of parameters: 85526016

[2023-11-23 11:42:08] [INFO] AnaLog will clear the previous Hessian, Storage, and Logging handlers after adding LoRA for gradient compression.

[2023-11-23 11:42:09] [INFO] Tracking the following modules:
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.0.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.0.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.0.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.0.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.0.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.0.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.1.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.1.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.1.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.1.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.1.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.1.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.2.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.2.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.2.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.2.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.2.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.2.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.3.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.3.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.3.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.3.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.3.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.3.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.4.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.4.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.4.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.4.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.4.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.4.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.5.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.5.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.5.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.5.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.5.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.5.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.6.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.6.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.6.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.6.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.6.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.6.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.7.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.7.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.7.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.7.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.7.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.7.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.8.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.8.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.8.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.8.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.8.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.8.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.9.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.9.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.9.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.9.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.9.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.9.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.10.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.10.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.10.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.10.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.10.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.10.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.11.attention.self.query.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.11.attention.self.key.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.11.attention.self.value.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.11.attention.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.11.intermediate.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.encoder.layer.11.output.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.bert.pooler.dense.analog_lora_B: Linear(in_features=64, out_features=64, bias=False)
[2023-11-23 11:42:09] [INFO] model.classifier.analog_lora_B: Linear(in_features=2, out_features=2, bias=False)
[2023-11-23 11:42:09] [INFO] Total number of parameters: 299012

